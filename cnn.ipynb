{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Birds Classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Transfer Learning\n",
    "\n",
    "Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks. This area of research bears some relation to the long history of psychological literature on transfer of learning, although formal ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.\n",
    "<br><br>\n",
    "Ressource from : [Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data starts here, we store the path to the training set into a variable for further referencing.\n",
    "\n",
    "train_path = './dataset/train'\n",
    "test_path = './dataset/test'\n",
    "valid_path = './dataset/valid'\n",
    "\n",
    "birds = np.array(list(os.listdir(train_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick only 20 type of birds to train on\n",
    "nr_birds = 20\n",
    "\n",
    "np.random.shuffle(birds)\n",
    "# slicing the data\n",
    "birds = birds[:nr_birds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'IBERIAN MAGPIE', 1: 'NOISY FRIARBIRD', 2: 'WHITE TAILED TROPIC', 3: 'RED BEARDED BEE EATER', 4: 'ROSY FACED LOVEBIRD', 5: 'BOBOLINK', 6: 'BAIKAL TEAL', 7: 'CHINESE BAMBOO PARTRIDGE', 8: 'HARLEQUIN DUCK', 9: 'FAIRY TERN', 10: 'AMERICAN PIPIT', 11: 'ROADRUNNER', 12: 'GAMBELS QUAIL', 13: 'EASTERN ROSELLA', 14: 'TOUCHAN', 15: 'ANTILLEAN EUPHONIA', 16: 'ALBERTS TOWHEE', 17: 'IBISBILL', 18: 'BLACK SKIMMER', 19: 'BIRD OF PARADISE'}\n"
     ]
    }
   ],
   "source": [
    "idx_to_name = {i:x for (i,x) in enumerate(birds)}\n",
    "name_to_idx = {x:i for (i,x) in enumerate(birds)}\n",
    "print(idx_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to store labels based on the folders.\n",
    "\n",
    "def get_data_labels(path, birds, dimensions):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for bird in birds:\n",
    "        imgs = [cv2.resize(cv2.imread(img), dimensions, interpolation=cv2.INTER_AREA) for img in glob.glob(path + \"/\" + bird + \"/*.jpg\")]\n",
    "        for img in imgs:\n",
    "            data.append(img)\n",
    "            labels.append(name_to_idx[bird])\n",
    "            \n",
    "    return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2936, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "data_train, labels_train = get_data_labels(train_path, idx_to_name.values(), (224,224))\n",
    "data_test, labels_test = get_data_labels(test_path, idx_to_name.values(), (224,224))\n",
    "data_valid, labels_valid = get_data_labels(valid_path, idx_to_name.values(), (224,224))\n",
    "\n",
    "print(data_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data = data / 255.0\n",
    "    data = data.astype('float32')\n",
    "    return data\n",
    "\n",
    "def one_hot(labels):\n",
    "    labels = np.eye(len(np.unique(labels)))[labels]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = normalize(data_train)\n",
    "data_test = normalize(data_test)\n",
    "data_valid = normalize(data_valid)\n",
    "\n",
    "labels_train = one_hot(labels_train)\n",
    "labels_test = one_hot(labels_test)\n",
    "labels_valid = one_hot(labels_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 19:13:32.562645: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-01 19:13:32.563146: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "weights_path = \"./dataset/pre-trained/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "base_vgg16 = VGG16(weights=weights_path, include_top=False, input_shape=(224, 224, 3))\n",
    "# base_vgg16.trainable = False\n",
    "base_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Freezing the weights\n",
    "for layer in base_vgg16.layers:\n",
    "    layer.trainable = False\n",
    " \n",
    "base_vgg16.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use “get_layer” method to save the last layer of the network\n",
    "last_layer = base_vgg16.get_layer('block5_pool')\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# flatten the classifier input which is output of the last layer of VGG16 model\n",
    "x = Flatten()(last_output)\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(nr_birds, activation='softmax', name='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " softmax (Dense)             (None, 20)                501780    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,216,468\n",
      "Trainable params: 501,780\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_vgg16.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaun/miniforge3/lib/python3.9/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "new_model.compile(Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 19:13:34.747276: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-01 19:13:35.305493: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935/2936 [============================>.] - ETA: 0s - loss: 1.1076 - accuracy: 0.6845"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 19:15:00.839021: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2936/2936 [==============================] - 90s 30ms/step - loss: 1.1078 - accuracy: 0.6843 - val_loss: 0.3271 - val_accuracy: 0.8900\n",
      "Epoch 2/10\n",
      "2936/2936 [==============================] - 164s 56ms/step - loss: 0.1845 - accuracy: 0.9608 - val_loss: 0.2897 - val_accuracy: 0.9000\n",
      "Epoch 3/10\n",
      "2936/2936 [==============================] - 162s 55ms/step - loss: 0.0630 - accuracy: 0.9905 - val_loss: 0.2678 - val_accuracy: 0.9100\n",
      "Epoch 4/10\n",
      "2936/2936 [==============================] - 192s 65ms/step - loss: 0.0238 - accuracy: 0.9973 - val_loss: 0.2674 - val_accuracy: 0.9200\n",
      "Epoch 5/10\n",
      "2936/2936 [==============================] - 182s 62ms/step - loss: 0.0095 - accuracy: 0.9993 - val_loss: 0.1096 - val_accuracy: 0.9500\n",
      "Epoch 6/10\n",
      "2936/2936 [==============================] - 182s 62ms/step - loss: 0.0075 - accuracy: 0.9980 - val_loss: 0.3764 - val_accuracy: 0.8900\n",
      "Epoch 7/10\n",
      "2936/2936 [==============================] - 189s 65ms/step - loss: 0.0041 - accuracy: 0.9997 - val_loss: 0.1282 - val_accuracy: 0.9400\n",
      "Epoch 8/10\n",
      "2936/2936 [==============================] - 207s 70ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.1387 - val_accuracy: 0.9400\n",
      "Epoch 9/10\n",
      "2936/2936 [==============================] - 200s 68ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.1698 - val_accuracy: 0.9500\n",
      "Epoch 10/10\n",
      "2936/2936 [==============================] - 211s 72ms/step - loss: 1.6885e-04 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='birds.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit(data_train, labels_train, steps_per_epoch=len(data_train),\n",
    "validation_data=(data_test, labels_test), validation_steps=3, epochs=10, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['val_accuracy'], 'b')\n",
    "plt.plot(history.history['val_loss'], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(model, data_valid, labels_valid):\n",
    "    predictions = model(data_valid)\n",
    "    wrong = 0\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if( np.argmax(pred) !=  np.argmax(labels_valid[i])):\n",
    "            wrong += 1\n",
    "    return (len(data_valid) - wrong) / len(data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "# we use the validation data to verify the accuracy\n",
    "accuracy = get_accuracy(new_model, data_valid, labels_valid)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-01 19:43:22.710186: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: birds-trained-model/assets\n"
     ]
    }
   ],
   "source": [
    "new_model.save('birds-trained-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architectures images from [Deep Learning for Vision Systems Book](https://www.manning.com/books/deep-learning-for-vision-systems)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50f518e30a1733e8e8c8b608de131219c9974acb0d02ce13c65eb2a815a608c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
